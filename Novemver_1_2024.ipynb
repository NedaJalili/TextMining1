{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM1W88AlUl+avB3jzkqwrkS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NedaJalili/TextMining1/blob/main/Novemver_1_2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BTDEV3dKILt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "sentence = \"In 2023, technology has advanced rapidly, allowing AI to assist in various tasks, from language translation to creative content generation.\"\n",
        "doc = nlp(sentence)\n",
        "print(f\"{'Token':<20}{'Is Alpha':<10}{'Is Digit':<10}\")\n",
        "for token in doc:\n",
        "  print(f\"{token.text:<20}{token.is_alpha:<10}{token.is_digit:<10}\")\n",
        "total_tokens = len(doc)\n",
        "alpha_tokens = sum(token.is_alpha for token in doc)\n",
        "digit_tokens = sum(token.is_digit for token in doc)\n",
        "print(\"\\n numbes of all tokens :\", total_tokens)\n",
        "print(\" number of alpha tokens :\", alpha_tokens)\n",
        "print(\"number of digit tikens :\", digit_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8kCIAOVRXS7",
        "outputId": "895fabc5-89a5-4f6d-ca5f-b4796c941e6d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token               Is Alpha  Is Digit  \n",
            "In                  1         0         \n",
            "2023                0         1         \n",
            ",                   0         0         \n",
            "technology          1         0         \n",
            "has                 1         0         \n",
            "advanced            1         0         \n",
            "rapidly             1         0         \n",
            ",                   0         0         \n",
            "allowing            1         0         \n",
            "AI                  1         0         \n",
            "to                  1         0         \n",
            "assist              1         0         \n",
            "in                  1         0         \n",
            "various             1         0         \n",
            "tasks               1         0         \n",
            ",                   0         0         \n",
            "from                1         0         \n",
            "language            1         0         \n",
            "translation         1         0         \n",
            "to                  1         0         \n",
            "creative            1         0         \n",
            "content             1         0         \n",
            "generation          1         0         \n",
            ".                   0         0         \n",
            "\n",
            " numbes of all tokens : 24\n",
            " number of alpha tokens : 19\n",
            "number of digit tikens : 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWROg-qrcUkI",
        "outputId": "e3ab0cee-d12f-4298-c2ae-0c2eafff3654"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wikipedia in /usr/local/lib/python3.10/dist-packages (1.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia-api\n",
        "import wikipediaapi\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "wiki_wiki = wikipediaapi.Wikipedia(\n",
        "language='en',\n",
        "user_agent='MyApp/1.0 (https://example.com; myemail@example.com)'\n",
        ")\n",
        "\n",
        "page = wiki_wiki.page(\"Geography\")\n",
        "summary = page.summary\n",
        "\n",
        "tokens = word_tokenize(summary)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "print(\"Tokens:\", ' '.join(tokens))\n",
        "print(\"Lemmatized Tokens:\", ' '.join(lemmatized_tokens))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bo4sDx89nNEH",
        "outputId": "ac2a460e-ee4a-44bc-e3f8-68b707a0b470"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia-api\n",
            "  Downloading wikipedia_api-0.7.1.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from wikipedia-api) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2024.8.30)\n",
            "Building wheels for collected packages: wikipedia-api\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.7.1-py3-none-any.whl size=14346 sha256=95e0162dd57815c0a218090eeeb279d6d2c61c5fd3c7777c77503b36153d763d\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/96/18/b9201cc3e8b47b02b510460210cfd832ccf10c0c4dd0522962\n",
            "Successfully built wikipedia-api\n",
            "Installing collected packages: wikipedia-api\n",
            "Successfully installed wikipedia-api-0.7.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: Geography ( from Ancient Greek γεωγραφία geōgraphía ; combining gê 'Earth ' and gráphō 'write ' ) is the study of the lands , features , inhabitants , and phenomena of Earth . Geography is an all-encompassing discipline that seeks an understanding of Earth and its human and natural complexities—not merely where objects are , but also how they have changed and come to be . While geography is specific to Earth , many concepts can be applied more broadly to other celestial bodies in the field of planetary science . Geography has been called `` a bridge between natural science and social science disciplines . '' Origins of many of the concepts in geography can be traced to Greek Eratosthenes of Cyrene , who may have coined the term `` geographia '' ( c. 276 BC – c. 195/194 BC ) . The first recorded use of the word γεωγραφία was as the title of a book by Greek scholar Claudius Ptolemy ( 100 – 170 AD ) . This work created the so-called `` Ptolemaic tradition '' of geography , which included `` Ptolemaic cartographic theory . '' However , the concepts of geography ( such as cartography ) date back to the earliest attempts to understand the world spatially , with the earliest example of an attempted world map dating to the 9th century BCE in ancient Babylon . The history of geography as a discipline spans cultures and millennia , being independently developed by multiple groups , and cross-pollinated by trade between these groups . The core concepts of geography consistent between all approaches are a focus on space , place , time , and scale . Today , geography is an extremely broad discipline with multiple approaches and modalities . There have been multiple attempts to organize the discipline , including the four traditions of geography , and into branches . Techniques employed can generally be broken down into quantitative and qualitative approaches , with many studies taking mixed-methods approaches . Common techniques include cartography , remote sensing , interviews , and surveying .\n",
            "Lemmatized Tokens: Geography ( from Ancient Greek γεωγραφία geōgraphía ; combining gê 'Earth ' and gráphō 'write ' ) is the study of the land , feature , inhabitant , and phenomenon of Earth . Geography is an all-encompassing discipline that seek an understanding of Earth and it human and natural complexities—not merely where object are , but also how they have changed and come to be . While geography is specific to Earth , many concept can be applied more broadly to other celestial body in the field of planetary science . Geography ha been called `` a bridge between natural science and social science discipline . '' Origins of many of the concept in geography can be traced to Greek Eratosthenes of Cyrene , who may have coined the term `` geographia '' ( c. 276 BC – c. 195/194 BC ) . The first recorded use of the word γεωγραφία wa a the title of a book by Greek scholar Claudius Ptolemy ( 100 – 170 AD ) . This work created the so-called `` Ptolemaic tradition '' of geography , which included `` Ptolemaic cartographic theory . '' However , the concept of geography ( such a cartography ) date back to the earliest attempt to understand the world spatially , with the earliest example of an attempted world map dating to the 9th century BCE in ancient Babylon . The history of geography a a discipline span culture and millennium , being independently developed by multiple group , and cross-pollinated by trade between these group . The core concept of geography consistent between all approach are a focus on space , place , time , and scale . Today , geography is an extremely broad discipline with multiple approach and modality . There have been multiple attempt to organize the discipline , including the four tradition of geography , and into branch . Techniques employed can generally be broken down into quantitative and qualitative approach , with many study taking mixed-methods approach . Common technique include cartography , remote sensing , interview , and surveying .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "import spacy\n",
        "import wikipediaapi\n",
        "\n",
        "wiki_wiki = wikipediaapi.Wikipedia(\n",
        "language='en',\n",
        "user_agent='MyApp/1.0 (https://example.com; myemail@example.com)'\n",
        ")\n",
        "\n",
        "page = wiki_wiki.page(\"Geography\")\n",
        "summary = page.summary\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "doc = nlp(summary)\n",
        "\n",
        "for token in doc:\n",
        "  print(f\"Token: {token.text}, Lemma: {token.lemma_}, POS: {token.pos_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIOGlN4TuDp4",
        "outputId": "39dd68fd-ba17-4ef1-a29c-aa361f8e1486"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.3)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.3)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Token: Geography, Lemma: Geography, POS: PROPN\n",
            "Token: (, Lemma: (, POS: PUNCT\n",
            "Token: from, Lemma: from, POS: ADP\n",
            "Token: Ancient, Lemma: Ancient, POS: PROPN\n",
            "Token: Greek, Lemma: Greek, POS: PROPN\n",
            "Token: γεωγραφία, Lemma: γεωγραφία, POS: NOUN\n",
            "Token: geōgraphía, Lemma: geōgraphía, POS: NOUN\n",
            "Token: ;, Lemma: ;, POS: PUNCT\n",
            "Token: combining, Lemma: combine, POS: VERB\n",
            "Token: gê, Lemma: gê, POS: PROPN\n",
            "Token: ', Lemma: ', POS: PUNCT\n",
            "Token: Earth, Lemma: earth, POS: NOUN\n",
            "Token: ', Lemma: ', POS: PUNCT\n",
            "Token: and, Lemma: and, POS: CCONJ\n",
            "Token: gráphō, Lemma: gráphō, POS: NOUN\n",
            "Token: ', Lemma: ', POS: PUNCT\n",
            "Token: write, Lemma: write, POS: NOUN\n",
            "Token: ', Lemma: ', POS: PUNCT\n",
            "Token: ), Lemma: ), POS: PUNCT\n",
            "Token: is, Lemma: be, POS: AUX\n",
            "Token: the, Lemma: the, POS: DET\n",
            "Token: study, Lemma: study, POS: NOUN\n",
            "Token: of, Lemma: of, POS: ADP\n",
            "Token: the, Lemma: the, POS: DET\n",
            "Token: lands, Lemma: land, POS: NOUN\n",
            "Token: ,, Lemma: ,, POS: PUNCT\n",
            "Token: features, Lemma: feature, POS: NOUN\n",
            "Token: ,, Lemma: ,, POS: PUNCT\n",
            "Token: inhabitants, Lemma: inhabitant, POS: NOUN\n",
            "Token: ,, Lemma: ,, POS: PUNCT\n",
            "Token: and, Lemma: and, POS: CCONJ\n",
            "Token: phenomena, Lemma: phenomena, POS: NOUN\n",
            "Token: of, Lemma: of, POS: ADP\n",
            "Token: Earth, Lemma: Earth, POS: PROPN\n",
            "Token: ., Lemma: ., POS: PUNCT\n",
            "Token: Geography, Lemma: geography, POS: NOUN\n",
            "Token: is, Lemma: be, POS: AUX\n",
            "Token: an, Lemma: an, POS: DET\n",
            "Token: all, Lemma: all, POS: ADV\n",
            "Token: -, Lemma: -, POS: PUNCT\n",
            "Token: encompassing, Lemma: encompass, POS: VERB\n",
            "Token: discipline, Lemma: discipline, POS: NOUN\n",
            "Token: that, Lemma: that, POS: PRON\n",
            "Token: seeks, Lemma: seek, POS: VERB\n",
            "Token: an, Lemma: an, POS: DET\n",
            "Token: understanding, Lemma: understanding, POS: NOUN\n",
            "Token: of, Lemma: of, POS: ADP\n",
            "Token: Earth, Lemma: Earth, POS: PROPN\n",
            "Token: and, Lemma: and, POS: CCONJ\n",
            "Token: its, Lemma: its, POS: PRON\n",
            "Token: human, Lemma: human, POS: ADJ\n",
            "Token: and, Lemma: and, POS: CCONJ\n",
            "Token: natural, Lemma: natural, POS: ADJ\n",
            "Token: complexities, Lemma: complexity, POS: NOUN\n",
            "Token: —, Lemma: —, POS: PUNCT\n",
            "Token: not, Lemma: not, POS: PART\n",
            "Token: merely, Lemma: merely, POS: ADV\n",
            "Token: where, Lemma: where, POS: SCONJ\n",
            "Token: objects, Lemma: object, POS: NOUN\n",
            "Token: are, Lemma: be, POS: AUX\n",
            "Token: ,, Lemma: ,, POS: PUNCT\n",
            "Token: but, Lemma: but, POS: CCONJ\n",
            "Token: also, Lemma: also, POS: ADV\n",
            "Token: how, Lemma: how, POS: SCONJ\n",
            "Token: they, Lemma: they, POS: PRON\n",
            "Token: have, Lemma: have, POS: AUX\n",
            "Token: changed, Lemma: change, POS: VERB\n",
            "Token: and, Lemma: and, POS: CCONJ\n",
            "Token: come, Lemma: come, POS: VERB\n",
            "Token: to, Lemma: to, POS: PART\n",
            "Token: be, Lemma: be, POS: AUX\n",
            "Token: ., Lemma: ., POS: PUNCT\n",
            "Token: While, Lemma: while, POS: SCONJ\n",
            "Token: geography, Lemma: geography, POS: NOUN\n",
            "Token: is, Lemma: be, POS: AUX\n",
            "Token: specific, Lemma: specific, POS: ADJ\n",
            "Token: to, Lemma: to, POS: ADP\n",
            "Token: Earth, Lemma: Earth, POS: PROPN\n",
            "Token: ,, Lemma: ,, POS: PUNCT\n",
            "Token: many, Lemma: many, POS: ADJ\n",
            "Token: concepts, Lemma: concept, POS: NOUN\n",
            "Token: can, Lemma: can, POS: AUX\n",
            "Token: be, Lemma: be, POS: AUX\n",
            "Token: applied, Lemma: apply, POS: VERB\n",
            "Token: more, Lemma: more, POS: ADV\n",
            "Token: broadly, Lemma: broadly, POS: ADV\n",
            "Token: to, Lemma: to, POS: ADP\n",
            "Token: other, Lemma: other, POS: ADJ\n",
            "Token: celestial, Lemma: celestial, POS: ADJ\n",
            "Token: bodies, Lemma: body, POS: NOUN\n",
            "Token: in, Lemma: in, POS: ADP\n",
            "Token: the, Lemma: the, POS: DET\n",
            "Token: field, Lemma: field, POS: NOUN\n",
            "Token: of, Lemma: of, POS: ADP\n",
            "Token: planetary, Lemma: planetary, POS: ADJ\n",
            "Token: science, Lemma: science, POS: NOUN\n",
            "Token: ., Lemma: ., POS: PUNCT\n",
            "Token: Geography, Lemma: geography, POS: NOUN\n",
            "Token: has, Lemma: have, POS: AUX\n",
            "Token: been, Lemma: be, POS: AUX\n",
            "Token: called, Lemma: call, POS: VERB\n",
            "Token: \", Lemma: \", POS: PUNCT\n",
            "Token: a, Lemma: a, POS: DET\n",
            "Token: bridge, Lemma: bridge, POS: NOUN\n",
            "Token: between, Lemma: between, POS: ADP\n",
            "Token: natural, Lemma: natural, POS: ADJ\n",
            "Token: science, Lemma: science, POS: NOUN\n",
            "Token: and, Lemma: and, POS: CCONJ\n",
            "Token: social, Lemma: social, POS: ADJ\n",
            "Token: science, Lemma: science, POS: NOUN\n",
            "Token: disciplines, Lemma: discipline, POS: NOUN\n",
            "Token: ., Lemma: ., POS: PUNCT\n",
            "Token: \", Lemma: \", POS: PUNCT\n",
            "Token: \n",
            ", Lemma: \n",
            ", POS: SPACE\n",
            "Token: Origins, Lemma: origin, POS: NOUN\n",
            "Token: of, Lemma: of, POS: ADP\n",
            "Token: many, Lemma: many, POS: ADJ\n",
            "Token: of, Lemma: of, POS: ADP\n",
            "Token: the, Lemma: the, POS: DET\n",
            "Token: concepts, Lemma: concept, POS: NOUN\n",
            "Token: in, Lemma: in, POS: ADP\n",
            "Token: geography, Lemma: geography, POS: NOUN\n",
            "Token: can, Lemma: can, POS: AUX\n",
            "Token: be, Lemma: be, POS: AUX\n",
            "Token: traced, Lemma: trace, POS: VERB\n",
            "Token: to, Lemma: to, POS: ADP\n",
            "Token: Greek, Lemma: Greek, POS: PROPN\n",
            "Token: Eratosthenes, Lemma: Eratosthenes, POS: PROPN\n",
            "Token: of, Lemma: of, POS: ADP\n",
            "Token: Cyrene, Lemma: Cyrene, POS: PROPN\n",
            "Token: ,, Lemma: ,, POS: PUNCT\n",
            "Token: who, Lemma: who, POS: PRON\n",
            "Token: may, Lemma: may, POS: AUX\n",
            "Token: have, Lemma: have, POS: AUX\n",
            "Token: coined, Lemma: coin, POS: VERB\n",
            "Token: the, Lemma: the, POS: DET\n",
            "Token: term, Lemma: term, POS: NOUN\n",
            "Token: \", Lemma: \", POS: PUNCT\n",
            "Token: geographia, Lemma: geographia, POS: NOUN\n",
            "Token: \", Lemma: \", POS: PUNCT\n",
            "Token: (, Lemma: (, POS: PUNCT\n",
            "Token: c., Lemma: c., POS: PROPN\n",
            "Token:  , Lemma:  , POS: SPACE\n",
            "Token: 276, Lemma: 276, POS: NUM\n",
            "Token: BC, Lemma: BC, POS: PROPN\n",
            "Token: –, Lemma: –, POS: PUNCT\n",
            "Token: c., Lemma: c., POS: PROPN\n",
            "Token:  , Lemma:  , POS: SPACE\n",
            "Token: 195/194, Lemma: 195/194, POS: PROPN\n",
            "Token: BC, Lemma: BC, POS: PROPN\n",
            "Token: ), Lemma: ), POS: PUNCT\n",
            "Token: ., Lemma: ., POS: PUNCT\n",
            "Token: The, Lemma: the, POS: DET\n",
            "Token: first, Lemma: first, POS: ADJ\n",
            "Token: recorded, Lemma: record, POS: VERB\n",
            "Token: use, Lemma: use, POS: NOUN\n",
            "Token: of, Lemma: of, POS: ADP\n",
            "Token: the, Lemma: the, POS: DET\n",
            "Token: word, Lemma: word, POS: NOUN\n",
            "Token: γεωγραφία, Lemma: γεωγραφία, POS: PROPN\n",
            "Token: was, Lemma: be, POS: AUX\n",
            "Token: as, Lemma: as, POS: ADP\n",
            "Token: the, Lemma: the, POS: DET\n",
            "Token: title, Lemma: title, POS: NOUN\n",
            "Token: of, Lemma: of, POS: ADP\n",
            "Token: a, Lemma: a, POS: DET\n",
            "Token: book, Lemma: book, POS: NOUN\n",
            "Token: by, Lemma: by, POS: ADP\n",
            "Token: Greek, Lemma: greek, POS: ADJ\n",
            "Token: scholar, Lemma: scholar, POS: NOUN\n",
            "Token: Claudius, Lemma: Claudius, POS: PROPN\n",
            "Token: Ptolemy, Lemma: Ptolemy, POS: PROPN\n",
            "Token: (, Lemma: (, POS: PUNCT\n",
            "Token: 100, Lemma: 100, POS: NUM\n",
            "Token: –, Lemma: –, POS: SYM\n",
            "Token: 170, Lemma: 170, POS: NUM\n",
            "Token: AD, Lemma: ad, POS: NOUN\n",
            "Token: ), Lemma: ), POS: PUNCT\n",
            "Token: ., Lemma: ., POS: PUNCT\n",
            "Token: This, Lemma: this, POS: DET\n",
            "Token: work, Lemma: work, POS: NOUN\n",
            "Token: created, Lemma: create, POS: VERB\n",
            "Token: the, Lemma: the, POS: DET\n",
            "Token: so, Lemma: so, POS: ADV\n",
            "Token: -, Lemma: -, POS: PUNCT\n",
            "Token: called, Lemma: call, POS: VERB\n",
            "Token: \", Lemma: \", POS: PUNCT\n",
            "Token: Ptolemaic, Lemma: Ptolemaic, POS: PROPN\n",
            "Token: tradition, Lemma: tradition, POS: NOUN\n",
            "Token: \", Lemma: \", POS: PUNCT\n",
            "Token: of, Lemma: of, POS: ADP\n",
            "Token: geography, Lemma: geography, POS: NOUN\n",
            "Token: ,, Lemma: ,, POS: PUNCT\n",
            "Token: which, Lemma: which, POS: PRON\n",
            "Token: included, Lemma: include, POS: VERB\n",
            "Token: \", Lemma: \", POS: PUNCT\n",
            "Token: Ptolemaic, Lemma: ptolemaic, POS: ADJ\n",
            "Token: cartographic, Lemma: cartographic, POS: ADJ\n",
            "Token: theory, Lemma: theory, POS: NOUN\n",
            "Token: ., Lemma: ., POS: PUNCT\n",
            "Token: \", Lemma: \", POS: PUNCT\n",
            "Token: However, Lemma: however, POS: ADV\n",
            "Token: ,, Lemma: ,, POS: PUNCT\n",
            "Token: the, Lemma: the, POS: DET\n",
            "Token: concepts, Lemma: concept, POS: NOUN\n",
            "Token: of, Lemma: of, POS: ADP\n",
            "Token: geography, Lemma: geography, POS: NOUN\n",
            "Token: (, Lemma: (, POS: PUNCT\n",
            "Token: such, Lemma: such, POS: ADJ\n",
            "Token: as, Lemma: as, POS: ADP\n",
            "Token: cartography, Lemma: cartography, POS: NOUN\n",
            "Token: ), Lemma: ), POS: PUNCT\n",
            "Token: date, Lemma: date, POS: NOUN\n",
            "Token: back, Lemma: back, POS: ADV\n",
            "Token: to, Lemma: to, POS: ADP\n",
            "Token: the, Lemma: the, POS: DET\n",
            "Token: earliest, Lemma: early, POS: ADJ\n",
            "Token: attempts, Lemma: attempt, POS: NOUN\n",
            "Token: to, Lemma: to, POS: PART\n",
            "Token: understand, Lemma: understand, POS: VERB\n",
            "Token: the, Lemma: the, POS: DET\n",
            "Token: world, Lemma: world, POS: NOUN\n",
            "Token: spatially, Lemma: spatially, POS: ADV\n",
            "Token: ,, Lemma: ,, POS: PUNCT\n",
            "Token: with, Lemma: with, POS: ADP\n",
            "Token: the, Lemma: the, POS: DET\n",
            "Token: earliest, Lemma: early, POS: ADJ\n",
            "Token: example, Lemma: example, POS: NOUN\n",
            "Token: of, Lemma: of, POS: ADP\n",
            "Token: an, Lemma: an, POS: DET\n",
            "Token: attempted, Lemma: attempt, POS: VERB\n",
            "Token: world, Lemma: world, POS: NOUN\n",
            "Token: map, Lemma: map, POS: NOUN\n",
            "Token: dating, Lemma: date, POS: VERB\n",
            "Token: to, Lemma: to, POS: ADP\n",
            "Token: the, Lemma: the, POS: DET\n",
            "Token: 9th, Lemma: 9th, POS: ADJ\n",
            "Token: century, Lemma: century, POS: NOUN\n",
            "Token: BCE, Lemma: BCE, POS: PROPN\n",
            "Token: in, Lemma: in, POS: ADP\n",
            "Token: ancient, Lemma: ancient, POS: ADJ\n",
            "Token: Babylon, Lemma: Babylon, POS: PROPN\n",
            "Token: ., Lemma: ., POS: PUNCT\n",
            "Token: The, Lemma: the, POS: DET\n",
            "Token: history, Lemma: history, POS: NOUN\n",
            "Token: of, Lemma: of, POS: ADP\n",
            "Token: geography, Lemma: geography, POS: NOUN\n",
            "Token: as, Lemma: as, POS: ADP\n",
            "Token: a, Lemma: a, POS: DET\n",
            "Token: discipline, Lemma: discipline, POS: NOUN\n",
            "Token: spans, Lemma: span, POS: NOUN\n",
            "Token: cultures, Lemma: culture, POS: NOUN\n",
            "Token: and, Lemma: and, POS: CCONJ\n",
            "Token: millennia, Lemma: millennia, POS: NOUN\n",
            "Token: ,, Lemma: ,, POS: PUNCT\n",
            "Token: being, Lemma: be, POS: AUX\n",
            "Token: independently, Lemma: independently, POS: ADV\n",
            "Token: developed, Lemma: develop, POS: VERB\n",
            "Token: by, Lemma: by, POS: ADP\n",
            "Token: multiple, Lemma: multiple, POS: ADJ\n",
            "Token: groups, Lemma: group, POS: NOUN\n",
            "Token: ,, Lemma: ,, POS: PUNCT\n",
            "Token: and, Lemma: and, POS: CCONJ\n",
            "Token: cross, Lemma: cros, POS: VERB\n",
            "Token: -, Lemma: -, POS: VERB\n",
            "Token: pollinated, Lemma: pollinate, POS: VERB\n",
            "Token: by, Lemma: by, POS: ADP\n",
            "Token: trade, Lemma: trade, POS: NOUN\n",
            "Token: between, Lemma: between, POS: ADP\n",
            "Token: these, Lemma: these, POS: DET\n",
            "Token: groups, Lemma: group, POS: NOUN\n",
            "Token: ., Lemma: ., POS: PUNCT\n",
            "Token: The, Lemma: the, POS: DET\n",
            "Token: core, Lemma: core, POS: NOUN\n",
            "Token: concepts, Lemma: concept, POS: NOUN\n",
            "Token: of, Lemma: of, POS: ADP\n",
            "Token: geography, Lemma: geography, POS: NOUN\n",
            "Token: consistent, Lemma: consistent, POS: ADJ\n",
            "Token: between, Lemma: between, POS: ADP\n",
            "Token: all, Lemma: all, POS: DET\n",
            "Token: approaches, Lemma: approach, POS: NOUN\n",
            "Token: are, Lemma: be, POS: AUX\n",
            "Token: a, Lemma: a, POS: DET\n",
            "Token: focus, Lemma: focus, POS: NOUN\n",
            "Token: on, Lemma: on, POS: ADP\n",
            "Token: space, Lemma: space, POS: NOUN\n",
            "Token: ,, Lemma: ,, POS: PUNCT\n",
            "Token: place, Lemma: place, POS: NOUN\n",
            "Token: ,, Lemma: ,, POS: PUNCT\n",
            "Token: time, Lemma: time, POS: NOUN\n",
            "Token: ,, Lemma: ,, POS: PUNCT\n",
            "Token: and, Lemma: and, POS: CCONJ\n",
            "Token: scale, Lemma: scale, POS: NOUN\n",
            "Token: ., Lemma: ., POS: PUNCT\n",
            "Token: \n",
            ", Lemma: \n",
            ", POS: SPACE\n",
            "Token: Today, Lemma: today, POS: NOUN\n",
            "Token: ,, Lemma: ,, POS: PUNCT\n",
            "Token: geography, Lemma: geography, POS: NOUN\n",
            "Token: is, Lemma: be, POS: AUX\n",
            "Token: an, Lemma: an, POS: DET\n",
            "Token: extremely, Lemma: extremely, POS: ADV\n",
            "Token: broad, Lemma: broad, POS: ADJ\n",
            "Token: discipline, Lemma: discipline, POS: NOUN\n",
            "Token: with, Lemma: with, POS: ADP\n",
            "Token: multiple, Lemma: multiple, POS: ADJ\n",
            "Token: approaches, Lemma: approach, POS: NOUN\n",
            "Token: and, Lemma: and, POS: CCONJ\n",
            "Token: modalities, Lemma: modality, POS: NOUN\n",
            "Token: ., Lemma: ., POS: PUNCT\n",
            "Token: There, Lemma: there, POS: PRON\n",
            "Token: have, Lemma: have, POS: AUX\n",
            "Token: been, Lemma: be, POS: AUX\n",
            "Token: multiple, Lemma: multiple, POS: ADJ\n",
            "Token: attempts, Lemma: attempt, POS: NOUN\n",
            "Token: to, Lemma: to, POS: PART\n",
            "Token: organize, Lemma: organize, POS: VERB\n",
            "Token: the, Lemma: the, POS: DET\n",
            "Token: discipline, Lemma: discipline, POS: NOUN\n",
            "Token: ,, Lemma: ,, POS: PUNCT\n",
            "Token: including, Lemma: include, POS: VERB\n",
            "Token: the, Lemma: the, POS: DET\n",
            "Token: four, Lemma: four, POS: NUM\n",
            "Token: traditions, Lemma: tradition, POS: NOUN\n",
            "Token: of, Lemma: of, POS: ADP\n",
            "Token: geography, Lemma: geography, POS: NOUN\n",
            "Token: ,, Lemma: ,, POS: PUNCT\n",
            "Token: and, Lemma: and, POS: CCONJ\n",
            "Token: into, Lemma: into, POS: ADP\n",
            "Token: branches, Lemma: branch, POS: NOUN\n",
            "Token: ., Lemma: ., POS: PUNCT\n",
            "Token: Techniques, Lemma: technique, POS: NOUN\n",
            "Token: employed, Lemma: employ, POS: VERB\n",
            "Token: can, Lemma: can, POS: AUX\n",
            "Token: generally, Lemma: generally, POS: ADV\n",
            "Token: be, Lemma: be, POS: AUX\n",
            "Token: broken, Lemma: break, POS: VERB\n",
            "Token: down, Lemma: down, POS: ADP\n",
            "Token: into, Lemma: into, POS: ADP\n",
            "Token: quantitative, Lemma: quantitative, POS: ADJ\n",
            "Token: and, Lemma: and, POS: CCONJ\n",
            "Token: qualitative, Lemma: qualitative, POS: ADJ\n",
            "Token: approaches, Lemma: approach, POS: NOUN\n",
            "Token: ,, Lemma: ,, POS: PUNCT\n",
            "Token: with, Lemma: with, POS: ADP\n",
            "Token: many, Lemma: many, POS: ADJ\n",
            "Token: studies, Lemma: study, POS: NOUN\n",
            "Token: taking, Lemma: take, POS: VERB\n",
            "Token: mixed, Lemma: mixed, POS: ADJ\n",
            "Token: -, Lemma: -, POS: PUNCT\n",
            "Token: methods, Lemma: method, POS: NOUN\n",
            "Token: approaches, Lemma: approach, POS: NOUN\n",
            "Token: ., Lemma: ., POS: PUNCT\n",
            "Token: Common, Lemma: common, POS: ADJ\n",
            "Token: techniques, Lemma: technique, POS: NOUN\n",
            "Token: include, Lemma: include, POS: VERB\n",
            "Token: cartography, Lemma: cartography, POS: NOUN\n",
            "Token: ,, Lemma: ,, POS: PUNCT\n",
            "Token: remote, Lemma: remote, POS: ADJ\n",
            "Token: sensing, Lemma: sensing, POS: NOUN\n",
            "Token: ,, Lemma: ,, POS: PUNCT\n",
            "Token: interviews, Lemma: interview, POS: NOUN\n",
            "Token: ,, Lemma: ,, POS: PUNCT\n",
            "Token: and, Lemma: and, POS: CCONJ\n",
            "Token: surveying, Lemma: survey, POS: VERB\n",
            "Token: ., Lemma: ., POS: PUNCT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"\"\"Apple Inc was founded by Steve Jobs, Steve Wozniak, and Ronald Wayne in Cupertino, California.\n",
        "The company became a huge success and introduced the iPhone in 2007. Tim Cook is currently the CEO of Apple.\n",
        "Microsoft, under Bill Gates, also played a major role in the tech revolution.\"\"\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "for ent in doc.ents:\n",
        "  print(f\"Text: {ent.text}, Label: {ent.label_}, Label Explanation: {spacy.explain(ent.label_)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5uKVKE6xNvY",
        "outputId": "3504370d-b685-4172-b0d4-97103757bd9b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Apple Inc, Label: ORG, Label Explanation: Companies, agencies, institutions, etc.\n",
            "Text: Steve Jobs, Label: PERSON, Label Explanation: People, including fictional\n",
            "Text: Steve Wozniak, Label: PERSON, Label Explanation: People, including fictional\n",
            "Text: Ronald Wayne, Label: PERSON, Label Explanation: People, including fictional\n",
            "Text: Cupertino, Label: GPE, Label Explanation: Countries, cities, states\n",
            "Text: California, Label: GPE, Label Explanation: Countries, cities, states\n",
            "Text: iPhone, Label: ORG, Label Explanation: Companies, agencies, institutions, etc.\n",
            "Text: 2007, Label: DATE, Label Explanation: Absolute or relative dates or periods\n",
            "Text: Tim Cook, Label: PERSON, Label Explanation: People, including fictional\n",
            "Text: Apple, Label: ORG, Label Explanation: Companies, agencies, institutions, etc.\n",
            "Text: Microsoft, Label: ORG, Label Explanation: Companies, agencies, institutions, etc.\n",
            "Text: Bill Gates, Label: PERSON, Label Explanation: People, including fictional\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "text = \"This is a simple example to demonstrate how stopwords can be extracted from a given text.\"\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "stopwords_in_text = [word for word in tokens if word.lower() in stop_words]\n",
        "\n",
        "print(\"Stopwords in text:\", stopwords_in_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjK0JHq7yVjp",
        "outputId": "b8e99175-0a6c-4af4-f6da-193e39f28c55"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopwords in text: ['This', 'is', 'a', 'to', 'how', 'can', 'be', 'from', 'a']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}